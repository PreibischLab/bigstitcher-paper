%\section{SUPPLEMENTARY METHODS}
\titleformat{\section}{\centering\normalfont\fontsize{13.5pt}{1em}\bfseries}{SUPPLEMENTARY NOTE \thesection: }{0em}{}
\section*{SUPPLEMENTARY NOTES}
\titleformat{\section}{\centering\normalfont\fontsize{11.5pt}{1em}\bfseries}{\thesection. }{0em}{}

\hspace{20mm}

\section{Data Import}
\label{sec:sup-note1}

\subsection*{Import of data}

\red{Microscopy acquisitions are saved in a multitude of vendor-specific formats, custom formats, and general formats such as TIFF stacks. We developed an extendable, user-friendly interface that automatically imports almost any format and extracts relevant metadata such as illumination directions, sample rotation, and approximate image positions using Bioformats\cite{bioformats}. The assignment of attributes to the image stacks in the raw data is usually automatic, or can be achieved with minimal interaction from the users. Therefore, the importer supports interactive placement of image tiles using regular grids or text file-based definitions (Supplementary Fig.~\ref{fig:sup-fig-manual-align1}). BigStitcher accesses image data through memory-cached, virtual loading\cite{imglib2}, optionally combined with virtual flatfield correction (Supplementary Fig.~\ref{fig:sup-fig-flatfield} and Supplementary Note~\ref{sec:flatfield}). Performance is optimal when images are stored using a multiresolution, blocked, compressed format enabling interactive visualization, processing and interaction with terabyte-sized image datasets. The importer therefore suggests by default to resave single-block images (e.g. TIFF) into the BigDataViewer HDF5 format\cite{pietzsch2015bigdataviewer}. Alternatively, by making use of cached ImgLib2 data structures, we support virtual loading of image planes from the raw files including caching of already loaded planes.}

\red{Data import is described in detail on the BigStitcher Wiki \url{https://imagej.net/BigStitcher_Define_new_dataset}. We additionally added an example youtube video that illustrates how the most generic import from TIFF stacks works in BigStitcher: \url{https://youtu.be/aUofNP6V0lg}. In case direct import from a custom format fails, we therefore suggest to manually re-save data as TIFF stacks and subsequently importing them into BigStitcher. It is important to preserve the calibration of the image stacks in the process.}

\subsection*{\texttt{SpimData} data format}

We internally represent our image data and metadata using an extended version of the \texttt{SpimData} data format of BigDataViewer\cite{pietzsch2015bigdataviewer}. Each image stack is defined by a (\texttt{ViewSetup}, \texttt{TimePoint})-combination. We extend the format by giving each \texttt{ViewSetup} the following \emph{attributes}: \texttt{Channel} to represent color channels, \texttt{Illumination} to represent illumination directions, \texttt{Angle} to represent multi-view acquisition angles and finally \texttt{Tile}, representing (local) x,y points in a multipoint acquisition.

In addition to those attributes, we store detected interest points, bounding boxes (named sub-volumes in which we can \emph{fuse} or deconvolve images), point spread functions for deconvolution and pairwise registrations (that have yet to be used in global optimization) for each (\texttt{ViewSetup}, \texttt{TimePoint}) \emph{view}. For each image stack, we also store its \emph{registration} (i.e. the transformation from pixel to world coordinates) as a list of affine transform matrices. The registration steps described below will typically prepend another transformation matrix to this list. Finally, the \texttt{SpimData} is associated with an \texttt{ImgLoader} object that can make image pixel data available as an ImgLib2 \cite{imglib2} \texttt{RandomAccessibleInterval} given a  (\texttt{ViewSetup}, \texttt{TimePoint}) \emph{view id}.

The \texttt{SpimData} data structure can be saved as an XML \emph{project file}, allowing users to manually edit it with any text editor. We automatically save previous versions of the project file to provide the user with the ability to un-do registration steps.

\begin{comment}
\subsection{Illumination selection}

When imaging large samples with multiple illumination directions,  a lot of unnecessary images are acquired since typically, only illumination from one direction provides optimal images. We therefore implemented a simple \emph{illumination selection} functionality in BigStitcher. It starts by \emph{combining} all (selected) images by their \texttt{Illumination} attribute, i.e. it groups images that share all other attributes besides \texttt{Illumination}. In each of the resulting groups we select a best image. We do this by loading the pixel data for all images in the group at the lowest resolution level (in the case of non-multiresolution images, this corresponds to the original image) and calculating a \emph{quality metric}. We currently offer mean intensity and mean gradient magnitude as quality metrics. The image with the highest score is kept, while all other images are marked as \emph{missing} in the \texttt{SpimData}, which will lead to them being ignored in subsequent processing steps. 
\end{comment}

\section{Flat-field correction}
\label{sec:flatfield}

Flat-field correction is the process of correcting for image artifacts due to uneven illumination or detection efficiency or fixed-pattern noise. Aside from being visually unpleasing, especially in tiled acquisitions, these artifacts can also effect image registration and downstream quantitative image analyses. We therefore offer simple on-the-fly correction for a \emph{dark image} (which might be nonzero due to e.g. camera offset) and a \emph{bright image} (representing uneven illumination or detection efficiency across the field-of-view). We calculate corrected pixel intensities $C$ from a raw image $R$ and bright and dark images $B$ and $D$ as:

\begin{equation}
\label{eq:flatfield-eq1}
C_{x} = \frac{(R_{x} - D_{x'}) * \overline{(B-D)} }{(B_{x'}- D_{x'})}
\end{equation}

The correction images can either have the same dimensionality as the raw images, in which case $x' = x$, or have lower dimensionality (e.g. when using 2D correction images on a 3D image stack), in which case $x' = (x_1 \hdots x_n)$ with $n$ being the dimensionality of the correction images. If a dark image is not provided by the user, we assume it to have constant intensity of 0 (corresponding to no background offset). Likewise, if no bright image is provided, we assume it to have constant intensity of 1 (uniform illumination and detection efficiency). 

We implemented the flat-field correction as a wrapper around an \texttt{ImgLoader}, calculating corrected pixel intensity values on-the-fly (with optional caching) every time an image is loaded. That way, the corrected images are available for all other processing steps such as intensity-based registration, interest point detection or image fusion, but it is still possible to activate or de-activate the correction or change bright or dark images after the initial flat-field correction. A separate (bright, dark)-correction image pair can be set for every image in the dataset by modifying the XML project file, while in the GUI we offer user-friendly assignment of correction images to every (channel, illumination direction)-pair.

\section{Pairwise shift calculation}

In BigStitcher, we currently offer three ways of calculating shifts between a pair of images: the Fourier-based \emph{phase correlation} algorithm, the Gradient-descent-based \emph{Lucas-Kanade} algorithm, both intensity-based methods, as well as interest point-based alignment.

\subsection*{Phase correlation}

By default, we calculate pairwise translational shifts of two images $I_1$ and $I_2$ using phase correlation \cite{preibisch2009globally, kuglin1975phase} using our new ImgLib2 implementation\cite{imglib2}. In noiseless images, the method produces a phase correlation matrix (PCM) $Q$ containing a single $\delta$-impulse at the location corresponding to the shift between the two images. Real images might contain multiple peaks (Supplementary Fig. \ref{fig:sup-fig-stitching}) and we localize the $n$ highest peaks in $Q$ by detecting peaks with subpixel accuracy using a n-dimensional implementation of a quadratic fit\cite{lowe2004distinctive}. Aside from allowing subpixel-accurate registration, we can use the precision obtained from the subpixel accuracy of the phase correlation to counteract the effects of downsampling (Supplementary Fig. \ref{fig:sup-fig-downsampling}), allowing us to achieve registration of similar quality to full-resolution with significant performance gains (Supplementary Fig. \ref{fig:sup-fig-downsampling-statistics-0}--\ref{fig:sup-fig-downsampling-statistics-2}). Due to the periodic nature of the Fourier shift theorem, each peak in the PCM actually correspond to $2^n$ possible shifts in $n$ dimensions. We therefore test each of these candidate shifts by calculating the cross-correlation between the images $I_1$ and $I_2$, optionally with interpolation in the case of sub-pixel shifts\cite{lowe2004distinctive}. We choose the shift vector~$t$ corresponding to the highest cross correlation as the final result after applying downsampling correction, if necessary.

It is often necessary to not only align two single images but groups of images, e.g. all channels of a tile. We therefore implemented a flexible framework for the registration of grouped images (see below). The two images $I_1$ and $I_2$ can have arbitrary affine pre-registrations such as sample rotation, correction of axial scaling, or already performed registration steps. If pre-registrations of $I_1$ and $I_2$ are identical, or are only based on different translations or axis-aligned scalings, we run the phase correlation on (downsampled) raw input images, otherwise on virtually fused images (Supplementary Note \ref{sec:fusion}).


%an ImgLib2 re-implementation of the Fourier-based \emph{phase correlation} algorithm \cite{preibisch2009globally}. In noiseless images, the method produces a phase correlation matrix $Q$ containing a single $\delta$-impulse at the location corresponding to the shift between the two images. Real images might contain multiple peaks, so we localize the $n$ highest peaks in $Q$. By detecting peaks with subpixel accuracy using a quadratic fit \cite{lowe2004distinctive}. Aside from allowing subpixel-accurate registration, we can also use this to counteract the effects of downsampling, allowing us to perform registration of similar quality to full-resolution with significant performance gains.
%
%Due to the periodic nature of the Fourier shift theorem, each peak in the PCM can actually correspond to $2^d$ possible shifts in $d$ dimensions. We therefore test each of these candidate shifts by calculating the cross-correlation between the images with $I2$ shifted according to the candidate shift (optionally with interpolation in the case of sub-pixel shifts). In the end, we keep the shift vector $t$ corresponding the highest cross correlation as the final result (applying downsampling correction, if necessary).

\subsection*{Lucas-Kanade}

In addition to the default phase correlation-based pairwise shift calculation, we offer registration via an ImgLib2 implementation of the \emph{inverse compositional} formulation of the gradient descent-based Lucas-Kanade optical flow algorithm \cite{baker2004lucas}. While the algorithm is applicable to a variety of transformation models, we currently stick to estimating a translation vector $t$. If the pairwise registration converges, we calculate the cross correlation of the overlapping portions of the images as a quality metric for the pairwise registration. 

\subsection*{Intensity-based Registration of grouped images}

In many use cases, one might want to align not single images but groups of images, e.g. all channels of a tile, in the pairwise registration step. For this, we implemented a flexible framework for the registration of grouped images.

Each attribute of the images can be set to be an \emph{axis of application}, an \emph{axis of comparison} or an \emph{axis of grouping}. The registration will proceed by first splitting the images by the application attributes, i.e. grouping all images that have the same value for these attributes. In each group, the images are then split by the comparison attributes and finally, the remaining image groups (that differ only in the grouping attributes) are combined into one image stack by either averaging all images for each grouping attribute or picking the image with a specific instance of the attribute.   

In a typical application, the stitching of tiled datasets, we would, for example, start by \emph{applying} the registration to all (\texttt{Angle}, \texttt{TimePoint})-combinations individually, \emph{comparing} by \texttt{Tiles} and finally \emph{grouping} by \texttt{Illumination} and \texttt{Channel} for each tile, e.g. by averaging illumination directions and picking a specific channel.

\subsection*{Intensity-based registration of images with pre-registrations}

The two images $I_1$ and $I_2$ can have arbitrary pre-registrations, i.e. pixel coordinates $x_{px}$ are mapped to world coordinates $x_w$ via the affine transforms $x_{w,I_1} = A _{I_1}x_{px,I_1} + b_{I_1}$ and $x_{w,I_2} = A _{I_2}x_{px,I_2} + b_{I_2}$. Depending on the values of $ A _{I_1}$ and $ A _{I_2}$, we consider two cases: If they are equal, i.e. the pre-registrations differ only by a translation, we perform the shift calculation on the raw pixel data of the overlapping volume to get a shift vector $t$ for $I_2$ in pixel coordinates. The transformation in world coordinates is then given by $R \bigl(\begin{smallmatrix}  & I & & t \\  0 & \cdots & 0 & 1 \end{smallmatrix}\bigr) R^{-1} $ with $R = \bigl(\begin{smallmatrix}  & A_{I_2} & & b_{I_2} \\  0 & \cdots & 0 & 1 \end{smallmatrix}\bigr)$. If the pre-registrations differ in more than just translation, we create virtually transformed images of the smallest rectangular bounding box enclosing the overlapping volume and use them as input to the registration. As the virtual input images are already in world coordinates in this case, the resulting transformation matrix for $I_2$ is simply $\bigl(\begin{smallmatrix}  & I & & t \\  0 & \cdots & 0 & 1 \end{smallmatrix}\bigr)$

\subsection*{Interest-point based}
\label{sssec:ip-registration}

For interest-point based pairwise registration, we detect local extrema in either Difference-of-Gaussian or Difference-of-mean filtered images, optionally followed by subpixel refinement of the detections via a quadratic fit \cite{lowe2004distinctive}. If we are registering a pair of image \emph{groups}, the interest points of each image in the group are pooled, with optional replacement of point clusters within a user-defined radius by their center.

For each image, we apply the current (affine) registrations to the pixel-coordinate interest points and then determine \emph{candidate point matches} via descriptor matching \cite{beads, descriptor}. We then perform model-based outlier removal via the RANSAC algorithm \cite{ransac}, yielding a set of \emph{inlier point pairs}, $C_{inliers}$, and an optimal translation $t$ for $I_2$, minimizing $\sum_{(ip_1, ip_2) \in C_{inliers} }{|| ip_1 - ip_2 - t||^2}$

\section{Global optimization}
\label{ssec:globalopt}

\subsection*{Estimation of globally optimal transformations}

The pairwise registration step results in \emph{links} between image (groups) $V$ (note that since we do not use the actual image \emph{content} in the global optimization, we will refer to the images by their integer \emph{id} in this section: $V \subset \mathbb{N}$). The links can be either in the form of pairwise transformations $T^{p}$  (such that coordinates $x$ from two images $V_i$ and $V_j$ can be transformed according to $T^p_{ij} (x_{j}) = x_i$) or \emph{point correspondences} $PM$ from which such transformations can be estimated.
The pairwise registrations thus form a \emph{link graph} $(V, C)$ with edges $C = \{(i,j) \in V \times V | T^p_{ij} \in T^p \}$ between image pairs for which we could determine pairwise transformations. Simply traversing a spanning tree of the link graph and propagating the pairwise transformations can lead to the compounding of pairwise registration errors, even if the traversal is done along a \emph{minimal} spanning tree determined according to some quality metric $q_{ij}$, e.g. cross-correlation, of the pairwise registrations.

We thus make use of an algorithm for globally optimal registration by iterative minimization of square displacement of point correspondences\cite{saalfeld2010rigid, saalfeld2012elastic} for reaching a reasonable consensus in this case. This point match-based framework allows for flexible grouping and fixing of images, is applicable to, among others, time series-, chromatic channel- or view-registration and can easily be adapted to incorporate the pairwise transformations from e.g. phase correlation. The algorithm is agnostic of the transformation model (e.g. translation, affine transform,...), with the only requirement being that the model parameters can be estimated by a least-squares fit from point correspondences.

We determine the globally optimal registrations $R$ given the image (groups) $V$, pairwise links $C$, pairwise $n$-dimensional point matches $PM$ with $PM_{ij} \subset \mathbb{R}^n \times  \mathbb{R}^n$ and a set of fixed views $F \subseteq V$ by minimizing:

\begin{equation}
\label{eq:globalopt-eq1}
\argmin_{R \setminus \{R_i | V_i \in F\}} \sum_{(i,j) \in C} \bigg( \sum_{(x_{k}, y_{k}) \in PM_{ij}} || R_{i}( x_{k}) - R_{j}( y_{k}) ||^2 \bigg)
\end{equation}

Note that for all fixed views, the registration will be constrained to be the identity transformation $I$: $\forall V_i \in F: R_{i} = I$.

\subsection*{Global optimization given pairwise transformations}

The intensity-based pairwise shift calculations do not directly give us the point correspondences we need for the global optimization step, instead the results are pairwise transformations $T^{p}$ in the form of affine transform matrices. We can, however, easily construct point correspondences by taking a set of points and transforming them with the \emph{inverse} transform (the only requirement being that the $n$-dimensional points do not all lie in a subspace of lower dimensionality of $\mathbb{R}^n$).  

Using the 3-dimensional pairwise transformations $T^{p}$ ($T^p_{ij} (x_{j}) = x_i$) between two image (groups) $V_i$ and $V_j$ given their existing registrations $R^{meta}$, we use the 8-point approximate bounding box of their overlapping region $BB_{ij}$ to construct the point correspondences: $PM_{ij} = \{ \big(bb_k, (T^p_{ij})^{-1}(bb_k) \big) | bb_k \in BB_{ij}\}$. We can then determine the globally optimal registrations $R$ by performing the minimization described above (\ref{eq:globalopt-eq1}).  

\subsection*{Global optimization with iterative link dropping}

Once the global optimization terminates due to convergence or exceeding of the maximum number of iterations, we can calculate the \emph{error} of the individual images as the average displacement of all interest points in an image to their point matches:

\begin{equation}
\label{eq:gloablopt-eq2}
e_i = \frac{\sum_{\{j: (i,j) \in C\}}  \sum_{(x_{k}, y_{k}) \in PM_{ij}} || R_{i}( x_{k}) - R_{j}( y_{k}) ||  }{ \sum_{\{j: (i,j) \in C\}} |PM_{ij}| }
\end{equation}

If the link graph $(V,C^n)$ contains links with contradicting point correspondences, stopping after one round of global optimization might leave us with unsatisfying results. In the \emph{iterative} version of the global optimization, we therefore check that both the average error of all images and the ratio of maximal and average error fall below a user-defined threshold. If these conditions are not yet met, we will proceed to iteratively remove disagreeing links from the link graph and repeat the global optimization. To do this, we first determine the link with the highest error by maximizing:

\begin{equation}
\label{eq:globalopt-eq3}
c_{worst} = \argmax_{(i,j)} \max_{(x_k, y_k) \in PM_{ij}}\bigg( (1-q_{ij})^2 \sqrt{d_{ijk}} \log_{10}\Big(\max\big(deg(i), \deg(j)\big)\Big)\bigg) 
\end{equation}

with $d_{ijk}$ denoting the distance of the $k$'th point match of the link $(i,j)$, $d_{ijk} = || R_{i}(x_k) - R_{j}(y_k) ||$, $\deg(i)$ denoting the degree (number of neighbors) of an image $V_i$ in the link graph and $q_{ij}$ being a \emph{quality metric} $\in (0,1)$ of the link, e.g. 0-truncated cross correlation. We then remove the worst link from the links ($C^{n+1} \leftarrow C^{n} \setminus c_{worst}$) and repeat the optimization step \ref{eq:globalopt-eq1} with the new link graph $(V, C^{n+1})$. The whole process is repeated until the errors fall below a user-defined threshold (in the worst case, links will be dropped until we end up with \emph{spanning trees} of the connected components in the link graph). 

\subsection*{Two-round global optimization using metadata}

If some cases, the link graph might contain multiple connected components, e.g. in datasets from screening applications, where the actual sample only occupies isolated "islands" and most images contain only background. In this case, we can only reliably determine pairwise transformations within the connected components and align images within the components in the global optimization step. We might, however, have reasonable registrations $R^{meta}$ from metadata and wish to keep as closely as possible to those if we do not have \emph{strong} links.

For this, we developed a \emph{two-round} version of the global optimization. In the first round, we determine registrations $R^{strong}$ as described above, using the graph of \emph{strong} links, i.e. links that are backed by pairwise transformations. In the second round, we determine the connected components in the $(V, C^{strong} )$ graph and a mapping $CC: \mathbb{N} \to \mathbb{N}$ from image (group) indices to  connected component indices as well as \emph{weak links} $C^{weak} = \{(i,j) \in V \times V | CC(i) \neq CC(j) \}$ between images in different components. We then determine transformations $R^{cc}$ for each connected component not containing a fixed image by minimizing:

\begin{equation}
\label{eq:globalopt-eq4}
\argmin_{R^{cc} \setminus \{ r^{cc}_{i} \in R^{cc} | CC_i \cap F \neq \emptyset \}} \sum_{(i,j) \in C^{weak}} \sum_{bb_{k} \in BB_{ij}} || R^{cc}_{CC(i)}\big( R^{strong}_{i}( bb_{k})\big) -  R^{cc}_{CC(j)}\big( R^{strong}_{j}( bb_{k}) \big) ||^2 
\end{equation}

Note that we use the corners $bb_k$ of the bounding box $BB_{ij}$ of the overlapping volume of two images $V_i$ and $V_j$ as the point correspondences. The overlap is determined according to the metadata transformations $R^{meta}$ and we essentially try to "un-do" the registrations of the first round as well as possible (while keeping the registrations \emph{within} the connected components). The final transformations $R$ are the concatenation of the registrations within the connected components with the relative transformations of the connected components: $R_{i} \leftarrow R^{cc}_{CC(i)} R^{strong}_{i}$.

\section{MultiView Registration}

For MultiView registration, e.g. registration of angles or time series stabilization, we first detect interest points in the individual images as described above (\ref{sssec:ip-registration}). Images may be grouped (and are by default if we are, e.g. registering tiled acquisitions from multiple angles for which we already aligned the tiles via an intensity-based method) according to their attributes, by pooling their interest points and optionally merging clusters.
%For registering time-series data, we offer four strategies. First, we can treat time points individually, registering only images within a time point. We can also perform interest point matching between different time points, either comparing all-to-all, all to a user-defined \emph{reference} time point or all time points within a defined range to each other.

Pairwise point correspondences can either be established by geometric local descriptor matching, a modified version of the iterative closest point (ICP\cite{icp}) algorithm or by simply matching the center of mass of the point clouds of both images (note that in this case the registration will be constrained to be a translation). Using the link graph $(V,C)$ and pairwise point correspondences $P_{ij}$ established thus, we calculate the final registration by performing global optimization as described above (\ref{ssec:globalopt}), optionally with iterative link removal and a second round to preserve metadata.

\subsection*{Geometric Local Descriptor Matching}

To identify corresponding interest points in between two point clouds, geometric local descriptor matching combined with random sample consensus (RANSAC\cite{ransac}) has been proven to be a powerful technique\cite{beads,descriptor}. The basic idea to express each interest point as a geometric constellation using its n (typically three) nearest neighboring interest points. The vector difference between two descriptors then describes how similar the local area of two points is. A geometric local descriptor (GLD) is assumed to be a correspondence candidate if it is at least m (typically one to ten) times more similar than the second most similar GLD\cite{lowe2004distinctive}. True corresponding interest points between two point clouds are finally identified using RANSAC on a regularized affine transformation model. However, fast GLD matching using the rotation-invariant technique based on geometric hashing\cite{beads} requires relatively randomly distributed points to robustly identify correspondences, while the non-accelerated, redundant, translation-invariant counterpart\cite{descriptor} identifies correspondences reliably in non-rotated point clouds of only up to a few thousand points in reasonable time. Here, we developed a new matching procedure by extending both techniques to better suit the requirements when attempting to identify corresponding interest point in between point clouds of prior unknow size derived from imaged structures that are potentially rotated relative to each other.

Redundancy is a powerful mechanism for GLD matching. It uses additional nearest neighbors but excludes some of them sequentially during matching making it more robust to potentially mis-detected interest points\cite{descriptor}. We therefore extend the fast rotation-invariant technique based on geometric hashing\cite{beads} with the capability for redundancy. This significantly increases the chance of being able to align randomly oriented point clouds very fast, albeit at low inlier ratios (ratio of true correspondences to total number of correspondence candidates). Rotation invariance is not desired if both point clouds are known to be approximately in same orientation, for example if the rotation of the sample performed by the microscope was known and has been applied to the dataset. Checking for potential rotations simply increases the chance for wrong correspondence candidates. We therefore implemented a fast translation-invariant GLD based on geometric hashing that supports redundancy. All four versions of GLD are available in BigStitcher to enable robust multi-view alignment.

\section{Non-Rigid Transformation}
\label{sec:nonrigid}

The underlying principle of moving least squares\cite{movingleastsquares} is to non-rigidly transform images using a set of corresponding points. Therefore, a local transformation is computed for each pixel using a distance-weighted fit of all corresponding points ensuring smoothness. In BigStitcher, corresponding points are a direct result of all interest point-based registration algorithms. To provide a sufficient amount of corresponding interest points, it is yet most useful to derive them using ICP\cite{icp}. Regularization is achieved on the registration side as corresponding interest points are identified on a regularized affine transformation model either using RANSAC\cite{ransac} or ICP\cite{icp}, which both specify a maximum error. This ensures that corresponding points cannot diverge more than this specified error from the regularized affine transformation of each image tile. In combination with virtual re-blocking, this error can be limited to smaller regions than the acquired, physical tiles. 

When computing local transformations for each image, it is necessary to ensure smoothness across n overlapping images by defining appropriate point correspondences. However, the registration identifies only pairwise correspondences in between pairs of images. From those, we therefore first identify all unique interest points across all images defined by all pairwise correspondences (Supplementary Fig. \ref{fig:non-rigid}). The location of each unique point is then determined by averaging the locations of all contributing interest points after applying their respective affine transformations. Thereby, the non-rigid transformation only accounts for the remaining error after affine alignment. For each image, corresponding points required for moving least squares are then subsequently defined between the unique point and the corresponding interest point of the transformed image only.

\section{Quality Estimation based on Fourier Ring Correlation}
\label{sec:frc}

For computing the 2d-Fourier Ring Correlation\cite{Nieuwenhuizen:2013ko} we adapted methods from the FRC ImageJ plugin\cite{frcij} as outlined in the Online Methods.  

\section{Simulation of light propagation in tissue using raytracing}
\label{sec:raytracing}

To describe the scene we will simulate we use two phantom images of the same size that separately define the visible light image (corresponding to fluorescent probe distribution) and the refractive indices map (Fig. 2b,c). We deliberately embed the spheroid-like object of varying refractive index ($R_i=1.1 – 1.21$) within a dense, invisible material with high refractive index ($R_i=1.1$) surrounded by air ($R_i=1.0$) to recapitulate significant aberrations in the illumination light path using a relatively small simulation volume of 289$\times$289$\times$289px. The object simulations are implemented in the multiview-simulations package\cite{mvdecon}.

We virtually scan a concave lightsheet (diameter of 1 pixel in the center, and 3 pixels at the edge) in 1-pixel steps and alternating left and right illumination through the sample (Fig. 2d), simulating an entire volume for each lightsheet position and direction (Fig. 2e and Supplementary Video 7). Therefore, we send 200.000 individual rays originating from random positions within the concave lightsheet through the sample for each lightsheet simulation. The initial vector of each ray points approximately along the lightsheet illumination direction and moves in 1-pixel steps through the volume. After each move we locally compute the Eigenvector of the largest Eigenvalue using the refractive index map, which defines the normal vector of the refractive surface at the current, sub-pixel ray position. Using this estimated refraction surface, we compute the refraction angle using raytracing algebra\cite{raytracing}, update the ray vector accordingly, and add a Gaussian distribution with an intensity defined by the visible light image to the simulation volume. For simplicity we ignore total reflection since it is mostly caused by numerical instabilities. We confirmed correct refraction of rays based on our computation of local Eigenvectors in discrete pixel-images by comparing it to refraction of rays in the corresponding continuous, parametric description of the same scene (not shown). 

The result of these simulations are 578 3d-volumes that recapitulate the principles of dual-sided lightsheet illumination (Fig. 2d). Inspired by classical raytracing, we perform a simplified detection simulation and therefore invert the ray path and only modulate signal intensity as a function of distance from the focal plane. Per camera pixel (289$\times$289) we send 500 rays at random positions within each pixel into the scene that are refracted as described above. For detection, we use the same the same refractive index map, and the result of each respective lightsheet illumination simulation serves as image data. However, since we assume an extremely high refractive index mismatch for illumination simulation to recapitulate the behavior in large samples, we assume a lower refractive index mismatch for the embedding material ($R_i=1.01$) to acquire reasonably distorted images. The relative refractive index mismatch within the spheroid-like object is conserved ($R_i=1.01 – 1.11$). We assume the focal point of the objective to lie in the center of the currently simulated lightsheet position. The light captured by each ray on its path through the sample is then computed as the sum of all light integrated when traveling through the sample, at each ray location  gaussian-weighted ($\sigma=3.5$) by the distance to the expected lightsheet position. The simulations were performed in parallel on the local compute cluster at the MDC.

Simulated data was created using the $net.preibisch.simulation.SimulateMultiViewAberrations$ class in the multiview-simulation package (release version 0.2.2). Since it is a Maven artifact, the versions of all dependencies are defined and the corresponding version can be built automatically from that source code state \small(\url{https://github.com/PreibischLab/multiview-simulation/commit/b41b74cce9287f804b670d7de3396605446818a8}).

\section{Quantification of Registration Quality}
\label{sec:quantreg}

\subsection*{Quantification of Image Stitching using Downsampling}

To assess the effect of downsampling on the pairwise stitching we use simulations of spheroid-like objects at different signal-to-noise ratios (SNRs) as ground truth. We create realistic images by mimicking image creation in light-sheet microscopy including optical sectioning, 3-fold anisotropy, light attenuation, convolution, and pixel intensity generation using Poisson processes\cite{mvdecon}. Importantly, pairs of overlapping images that we use for benchmarking the subpixel phase correlation method are created using different Poisson processes and are additionally rendered with half a pixel offset of the full resolution images to avoid nearly identical overlaps at high SNRs due to the simulation process (Supplementary Fig. \ref{fig:sup-fig-downsampling}).

We simulate 500 pairwise overlaps, each at SNRs ranging from 1 to 32, and lateral downsamplings ranging from 1$\times$ to 8$\times$, where axial downsampling is matched as good as possible to achieve near-isotropic resolution as in the actual software. We illustrate that across SNRs downsampled images yield a constant registration quality, which even exceeds that of registration at full resolution for low SNRs. This is achieved through a combination of the smoothing effect during downsampling (Supplementary Fig. \ref{fig:sup-fig-downsampling}) and precise subpixel-localization (Supplementary Fig. \ref{fig:sup-fig-downsampling-statistics-0}--\ref{fig:sup-fig-downsampling-statistics-2}). Due to the smoothing effect, registration quality therefore initially increases at 2-fold and 4-fold downsampling (Supplementary Fig. \ref{fig:sup-fig-downsampling-statistics-0}--\ref{fig:sup-fig-downsampling-statistics-2}), while when using more downsampling, the loss of pixel resolution outweighs the effect of smoothing and hence the quality drops. Registrations with a constant quality of an average error of below one pixel can be computed at a fraction of the computing time compared to full resolution, typically 4 - 115 times faster. Existing outliers are filtered during global optimization and overall registration quality can further be improved during the ICP\cite{icp }refinement step.

Simulated data was created using the $net.preibisch.stitcher.headless.StitchingPairwise$ class in the BigStitcher package (release version 0.3.3). Since it is a Maven artifact, the versions of all dependencies are defined and the corresponding version can be built automatically from that source code state \small(\url{https://github.com/PreibischLab/BigStitcher/commit/0d7f79a59ab15fb1805157ab72c5bc9802b02fbd}).

\subsection*{Quantification of overall Registration Quality}

To quantify registration quality, we acquired an as-small-as-possible (166GB), cleared section of an adult mouse brain. It is imaged at lower magnification from two angles (0${}^\circ$ and 180${}^\circ$) and in a 2$\times$3 tile configuration with dual-sided illumination for each angle (Fig. 2k,l, Supplementary Fig. \ref{fig:sup-fig-illu-select},\ref{fig:sup-fig-registration-quality} and Supplementary Table \ref{tab:datasets}).

We identified a ground-truth set of corresponding interest points in directly adjacent images by manually selecting bright spots from a set of interest points that were automatically detected using Difference-of-Gaussian filtering and subpixel-accurate local maxima determination (Suppl. Fig. \ref{fig:sup-fig-registration-quality}a,b). For each image pair, we selected between 19 and 52 corresponding points, in total 692.

We then registered the dataset in BigStitcher for tiled acquisition only, tiled acquisition across illumination directions, and for the multi-tile, dual illumination, multi-view case. This is achieved by grouping the images either by angle and illumination direction, just by angle, or not at all. 
For the single-view cases, we performed translational alignment by stitching the images using phase correlation. For an all-to-all registration with a translation model, the images of angle 2 were manually rotated by 180${}^\circ$ and then all images were aligned using interest points by fast translation-invariant GLD matching followed by RANSAC\cite{ransac} and global optimization using a translation model. All translation-model alignments were refined using ICP\cite{icp} as described above. The point correspondences determined during ICP were further used for non-rigid refinement.

For virtual re-blocking, each original image was split into 2$\times$2$\times$2 sub-blocks (with 120px overlap in xy and 100px overlap in z). After the re-blocking, 4--28 manually selected point correspondences remained between each set of directly adjacent blocks. For each of the image groupings and registration models used, we calculated an average error of the manually selected point correspondences:

\begin{equation}
\label{eq:regQuality}
e_{avg} = \frac{1}{|I|} \sum_{i_1 \in I} \bigg( \frac{1}{|C(i_1)|} \sum_{i_2 \in C(i_1)} \Big( \frac{1}{|PM(i_1, i_2)|} \sum_{(p_1, p_2) \in PM(i_1, i_2)} || T^{i_1}(p_1) - T^{i_2}(p_2) ||^2 \Big) \bigg)    
\end{equation}

With $I$ being the set of images, $C(i)$ the adjacent images of an image $i$ (ignoring diagonal pairs for which no corresponding interest points were manually selected as well as pairs that are not in the same group, e.g. when grouping by angle and illumination direction, and pairs from the same original image in the virtually blocked dataset), $PM(i,j)$ the corresponding manually selected interest points of images $i,j$ and $T_i$ the transformation of image $i$.

To estimate the lowest theoretically achievable errors given a certain transformation, we use only the manually selected point correspondences to calculate a globally optimal registration (and optionally the non-rigid refinement thereof) of the images and then calculate the average error from the same point correspondences as described above. In the virtually blocked case, we also use manually selected point correspondences (826 in total) between adjacent blocks within the same original image for the registration (but ignore them for the final error calculation). 

Despite relatively small aberrations in this sample as compared to entire mouse brains (compare with Fig. 2f,g,h and Fig. 2m) we illustrate that using only translation as transformation model is only reasonable for tiled acquisitions that do not include multiple illuminations or multiple acquisition angles, yet even there spherical aberrations persist that question the standard use of translation models in general (Supplementary Fig. \ref{fig:sup-spherical},\ref{fig:sup-fig-icp}). The alignment errors increase when aligning across illumination directions and greatly increase when aligning different acquisition angles. Importantly, please note that the alignment quality across different illumination directions is significantly reduced on larger samples when using only translation models (compare Fig. 2f,g,h and Fig. 2m). Using the affine, split-affine or non-rigid registration functionality, BigStitcher can sharply reduce the registration errors in large cleared and expanded samples. As a trade-off between speed and quality we usually choose affine or split-affine registrations.

\section{Image Fusion}
\label{sec:fusion}

We \emph{fuse} multiple images by performing a weighted average of the raw images $I^{raw}$ transformed by their registrations $R$. Each raw image $I^{raw}_i$ has a set weight images $W_i$. For example, we allow the user to weigh the images with a cosine-shaped fade-out, de-emphasizing the artifact-prone border regions of the individual images, as well as by the approximate local entropy, to emphasize images with sharper structures in overlapping regions. Since the raw images will be evaluated at non-integer coordinates, we offer the choice between nearest-neighbor and linear interpolation. Downsampling can easily be achieved by prepending a scaling transformation to each of the registrations $R$. The intensity of the fused volume at a coordinate $x$ is given by:

\begin{equation}
\label{eq:fusion-eq1}
I^{fused}(x) ={ \frac{  \sum_{ I^{raw}_i \in I^{raw}} \Big(I^{raw}_i\big(R_i^{-1}(x)\big) * \prod_{w_j \in W_i}{w_j\big(R_i^{-1}(x)\big)\Big)}}{ \sum_{I^{raw}_i \in I^{raw}}\Big( \prod_{w_j \in W_i}{w_j\big(R_i^{-1}(x)\big)\Big)}}}
\end{equation}

In practice, we evaluate $I^{fused}$ only at integer coordinates of a user-defined \emph{bounding box}. We implemented the image fusion to perform all calculations virtually on-the-fly, with caching of previously computed planes using imglib2-cache. This allows the quick inspection of fusion results as well as creation and planewise saving of images that might exceed the RAM available to the user.

\section{Brightness and Contrast adjustment}
\label{sec:brightness-adjust}

Even after correcting for fixed-pattern noise (\ref{sec:flatfield}), differences in brightness and contrast between images, e.g. due to bleaching, might persist and be visible in the fused images. To correct for this, we estimate optimal linear transforms of pixel intensities in adjacent images \cite{blasse2017premosa} to achieve uniform brightness and contrast in the whole dataset. We minimize the intensity difference of all pixels in the overlapping volume $O_{AB}$ of two images $I_A, I_B$ (with corresponding coordinates $(x_A, x_B)$ according to the current registrations): 

\begin{equation}
\label{eq:brighness-adjust-1}
\argmin_{\alpha, \beta} \sum_{I_A  \in I}  \bigg( \sum_{I_B \in I \setminus I_A} \Big( \sum_{(x_A, x_B) \in O_{AB}} \big( I_B(x_B) - \left[ \alpha^{I_A} I_A(x_A) + \beta^{I_A} \right] \big)^2 \Big)\bigg)
\end{equation}

Since this is equal to one-dimensional point correspondence registration, we can make use of the same iterative optimization algorithm used for image registration (\ref{ssec:globalopt}). To reduce influence of noise and computational costs, we use (precomputed) downsampled versions of the images for the optimization. A problem with unconstrained optimization is the possibility of convergence to the trivial solution of setting all pixel intensities to zero. We therefore formulate the linear transform $I(x)*\alpha + \beta$ as a weighted average between a linear transform, an additive transform and the identity transform:

\begin{equation}
\label{eq:brighness-adjust-2}
\alpha I(x) + \beta = \lambda_1 * (\alpha' I(x) + \beta_1) + \lambda_2 * (I(x) + \beta_2) + \lambda_3 * I(x) 
\end{equation}

with user-definable regularization parameters $\lambda_1, \lambda_2, \lambda_3: \lambda_1+ \lambda_2+\lambda_3 = 1$. By using nonzero $\lambda_2, \lambda_3$, we can constrain the optimization to not converge to the trivial solution.

The size of overlaps between image tiles can differ significantly. Therefore, intensity transformations supported by many overlapping pixels will implicitly have a higher weight, which can lead to the fact that visible intensity differences between tiles with little overlap persist. To compensate this effect we allow to balance overlaps by setting a maximal number of corresponding pixels. To ensure equal distribution of these corresponding pixels, we randomly remove pixels from the set of all pixels until the desired number is achieved.

\section{PSF Measurement and PSF Extraction}
\label{sec:psf}

In light-sheet microscopy, measured PSFs often differ significantly from simulated ones due to variable precision of light-sheet alignment in every experiment. Therefore, light-sheet deconvolution usually relies on the extraction of PSFs from the actual experiment19,25. To be able to perform PSF extraction in cleared tissue we developed a new protocol. Estapor Fluorescent Microspheres (F-XC 030) were diluted 1:20000 in monomer solution containing bis-acrylamide (0,05$\%$ v/v bis-acrylamide, 4$\%$ v/v acrylamide, 4$\%$ w/v Paraformaldehyde (PFA), 0.25$\%$ w/v VA-044 in PBS). The monomer solution was polymerized under constant vacuum and shaking at 37${}^\circ$C for 2 hours.  The formed hydrogel was incubated in FocusClear overnight and imaged using the Zeiss Lightsheet Z.1 microscope with the same experimental settings used to acquire previous samples. For C. elegans dauer imaging fixed larvae were embedded in 1.2$\%$ agarose together with Estapor Fluorescent Microspheres (F-Z 030), diluted 1:2000. For ExM data acquired on the IsoView microscope depth-sectioned images (0.4125 $\mu$m step) of fluorescent beads (200nm diameter) embedded in 0.6$\%$ low-melting-temperature agarose were imaged using the same experimental settings as for sample imaging. For all samples, PSFs were extracted by detecting interest points in the acquired bead images. Potential bead aggregates were excluded by manual removal on the maximum intensity projection using the BigStitcher module “Manage Interest Points $>$ Remove Interactively”.

\section{MultiView Deconvolution}
\label{sec:mvdecon}

In addition to real-time image fusion, we offer deconvolution of bounding-box-defined volumes using a multi-view formulation of the iterative Richardson-Lucy deconvolution algorithm\cite{richardson, lucy} with Tikhonov regularization\cite{Tikhonov/Arsenin/77} and various optimizations\cite{mvdecon}. The PSFs required for deconvolution can be extracted from interest points detected in the images (e.g. when subdiffraction fluorescent beads were incorporated with the sample, see section \ref{sec:psf}) or supplied as TIFF stacks with odd dimensions by the user. BigStitcher offers GPU acceleration of the deconvolution on CUDA-capable Nvidia GPUs.

To allow deconvolution of multi-tile views, we extended the original deconvolution\cite{mvdecon} to be based on the virtual fusion. Thereby, any number of input image tiles are virtually fused and serve as one of input views for the multi-view deconvolution. Proper multi-view deconvolution of partly overlapping samples requires sophisticated weight normalization in between views\cite{mvdecon}, which we implemented to be computed virtually. Since also the input views are also virtually loaded, the memory requirement of the deconvolution solely depends on the output image size and shows a significantly increased memory-efficiency. All virtual inputs and weights are additionally cached, ensuring highest-possible processing performance for systems with large amounts of RAM.

\section{Imaging}
\label{sec:imaging}

3D images of cleared mouse brains were imaged using the Zeiss Lightsheet Z.1 microscope. Each sample was attached to the sample holder using a cyanoacrylate-based glue. The mounted sample was placed in the FocusClear pre-filled imaging chamber. Images were acquired using the EC Plan-NEOFLUAR 5$\times$/NA 0.16 objective together with the LSFM 5$\times$/NA 0.1 illumination objectives on a Zeiss Light-sheet Z.1. The data was acquired using dual side illumination and from different angles. Images were collected with two 1920$\times$1920 pixel sCMOS cameras and stored in the Zeiss CZI file format.

Fixed C. elegans dauer larvae were embedded in 1.2$\%$ agarose containing fluorescent beads and imaged using the same microscope in a water-filled sample chamber. Imaging was performed using the 20$\times$/NA 1.0 objective with additional 2$\times$ zoom. 

3D images from a cleared and expanded central nervous system of a Drosophila 1st instar larva were acquired using an IsoView light-sheet microscope\cite{isoview} that has been modified for multi-tile acquisition. To prepare the sample for imaging, excess gel surrounding the expanded sample was removed using a scalpel, leaving four flat and smooth gel surfaces for imaging. Some extra gel was left underneath the sample for mounting, and the sample was affixed to a cylindrical post using a cyanoacrylate-based glue. The mounted sample was placed in the imaging chamber filled with deionized water. Orthogonal views for each tile of the sample were acquired sequentially by switching the illumination and detection orders in IsoView. Images were acquired using SpecialOptics 16$\times$/NA 0.71 objectives and recorded using full frames (2048$\times$2048 pixels, pixel pitch of 0.4125~$\mu$m in sample space) of Orca Flash 4.0 v2 sCMOS cameras. The sample was held stationary during multi-view acquisition of each tile, and depth-sectioned images were acquired every 0.8125 $\mu$m by translating the detection piezos over a range of 750~$\mu$m. A tile for each view thus covered a field of 832~$\mu$m (X), 832~$\mu$m (Y), and 750~$\mu$m (Z). Automated tiling across the entire sample was achieved by moving the sample in predefined steps of 600~$\mu$m in X, Y, and Z until full coverage was achieved. Bi-directional light-sheet illumination was achieved using opposing SpecialOptics objectives and the illumination NA was chosen to be 0.0315 for a confocal parameter of approximately 416~$\mu$m. The light-sheets from opposing arms were shifted approximately by their Rayleigh length (208 $\mu$m) toward the illumination objectives so that each light-sheet provided uniform coverage of the respective half in the acquired image. 

A summary of the most important acquisition parameters can be found in Supplementary Table \ref{tab:datasets}.

\section{Clearing}
\label{sec:clearing}

Clearing of brain tissue was performed using the CLARITY protocol\cite{clarity}. Mice were deeply anesthetized by intraperitoneal injection of 100 mg/kg Ketamine and 15 mg/kg Xylazine. Mice were exsanguinated by transcardial perfusion with 25 ml cold PBS followed by whole body perfusion with 25 ml cold monomer solution (4$\%$ v/v acrylamide, 4$\%$ w/v Paraformaldehyde (PFA), 0.25$\%$ w/v VA-044 in PBS). The brains were collected and fixed in monomer solution for 2 more days. Next, the whole brains were placed in fresh monomer solution and oxygen was removed from the tubes by vacuum and flushing the tube with nitrogen gas for 15 minutes. The samples were then polymerized by placing the tubes in a 37${}^\circ$C water bath under gentle shaking for 2 hours. Polymerized brains were placed in clearing solution (4$\%$ SDS in 200 mM Boric acid). Brains were incubated in clearing solution for 1 week at 37${}^\circ$C with daily solution change. Then, the brains were actively cleared using the X-Clarity setup from Logos Bioscience for 24 hours with a current of 1 A at 37${}^\circ$C. Cleared brains were washed twice overnight with 0.1$\%$ v/v Triton X-100 in PBS and once with PBS.


\section{Expansion Microscopy}
\label{sec:expansion}

\subsection*{Expansion Microscopy Sample Preparation}

For Expansion Microscopy (ExM), the nervous system of a 1st instar Drosophila larva of was extracted, fixed in 4$\%$ PFA/1xPBS/0.1$\%$Triton for 1 hour and washed 2x10 min in 1xPBS/0.1$\%$ Triton. Before each antibody usage, the nervous system and the antibodies were blocked in 5$\%$ goat serum/1xPBS/0.1$\%$ Triton for one hour. Following the blocking, the nervous system was incubated overnight at 4${}^\circ$C in 1:500 monoclonal Anti-$\alpha$-Tubulin antibody produced in mouse (Sigma Aldrich T6199 1mg/ml). After 5x10 min washing (1xPBS/0.1$\%$ Triton), the secondary antibody 1:250 Anti-Mouse CF™488A antibody produced in goat (Sigma Aldrich AB4600387 2mg/ml) was added overnight at 4${}^\circ$C. 

\subsection*{Detailed Expansion Microscopy Protocol}

\red{Acryloyl-X, SE (6-((acryloyl)amino)hexanoic acid, succinimidyl ester; here abbreviated AcX; Thermo-Fisher) was resuspended in anhydrous DMSO at a concentration of 10 mg/mL, aliquoted and stored frozen in a desiccated environment. AcX stock solution was diluted in 1xPBS to a final concentration of 0.1 mg/mL AcX. Specimens were incubated in this 0.1mg/mL AcX solution for $>$6 h, at RT. Monomer solution (1xPBS, 1M NaCl, 1.84M sodium acrylate, 0.35M acrylamide, 3.2mM N,N'-methylenebisacrylamide) was mixed, frozen in aliquots, thawed fully, vortexed, and cooled to 4${}^\circ$C before use. Concentrated stocks of the initiator ammonium persulfate (APS, 10$\%$ w/w), accelerator tetramethylethylenediamine (TEMED, $10\%$ v/w) and inhibitor 4-hydroxy-2,2,6,6-tetramethylpiperidin-1-oxyl (4-HT, 0.5$\%$ w/w) were prepared as concentrated stock solutions, which were frozen in aliquots and then fully thawed and vortexed before use. Initiator, accelerator and inhibitor stock solutions were added to the monomer solution at a ratio of 2uL each per 94uL monomer solution to produce complete monomer solution. Specimens were washed 2x15min in complete monomer solution, on ice with shaking. Specimens were transferred to 3D-printed gelation chambers sized 1cm x 1cm and 0.3mm deep, along with 30uL of complete monomer solution. Chamber was covered with a cover glass and transferred to a humidified 37${}^\circ$C incubator for 2hr for gelation and gel curing.}

\red{Proteinase K (New England Biolabs) was diluted 1:100 to 8 units/mL in digestion buffer (50 mM Tris (pH 8), 1 mM EDTA, 0.5$\%$ Triton X-100, 1 M NaCl) to produce proteinase solution. Gel was recovered from chamber and incubated fully immersed in proteinase solution overnight at RT, with shaking. The digested gel was next incubated in at least a 10-fold excess volume of monomer solution with accelerator and inhibitor (no initiator) 2x15min, followed by complete monomer solution 2x15min on ice, with shaking. (Initiator is omitted from the first two washes to prevent premature gelation.) During incubation, a glass slide and cover glass are coated with parafilm by laying parafilm with paper backing onto the glass surface (parafilm down) and scraping a razor blade across the backing, then removing the backing. A 5mL syringe filled with silicone grease was used to apply four dabs (approx. 50uL each) of grease to the glass slide, at the corners of a rectangle slightly smaller than the cover glass. The gel was transferred to the coated slide and excess fluid was removed. The cover glass was placed over the gel, parafilm side down, contacting the dabs of grease. The cover glass was gently pressed down, squeezing the grease, until the coverglass contacted and sat flat across the gel. Excess complete monomer solution was backfilled into the resulting chamber to impede access of atmospheric oxygen to the gel. The completed chamber was moved to the 37${}^\circ$C humidified incubator for 2hr for gelation and curing.}

\red{The resulting doubly-gelled specimen was recovered from the chamber, and excess gel was trimmed away. The trimmed double gel was washed in excess volumes of doubly deionized water for 0.25-2 h to expand. This step was repeated 3-5 times in fresh water, until the size of the expanding sample plateaued.}


\begin{comment}
\section{Headless operation}

In addition to the graphical user interface (GUI), we offer ImageJ \texttt{Plugin}s for most of the individual steps, such as data import, illumination selection, pairwise shift calculation, link filtering, global optimization and image fusion/deconvolution. The results will not be displayed interactively but saved to the XML project file or output files immediately. The individual steps can be recorded as ImageJ \emph{macros} \cite{imagej, macro-operation} and easily combined into a script for headless batch processing. 
\end{comment}
